---
{"dg-publish":true,"dg-permalink":"books/36632126/Data-Aggregation-and-Group-Operations","permalink":"/books/36632126/Data-Aggregation-and-Group-Operations/","metatags":{"description":"本书第 1 版出版于 2012 年，彼时基于 Python 的开源数据分析库（例如 pandas）仍然是一个发展迅速的新事物，本书也成为该领域排名 No 1 的经典畅销书，前两版中文版累计销售近 30 万册。第 3 版针对 Python 3.10 和 pandas 1.4 进行了更新，并通过实操讲解和实际案例向读者展示了如何高效地解决一系列数据分析问题。读者将在阅读过程中学习新版本的 pandas、NumPy、IPython 和 Jupyter。本书作者 Wes McKinney 是 Python pandas 项目的创始人。本书对 Python 数据科学工具的介绍既贴近实战又内容新颖，非常适合刚开始学习 Python 的数据分析师或刚开始学习数据科学和科学计算的 Python 程序员阅读。","og:site_name":"DavonOs","og:title":"利用 Python 进行数据分析 (原书第3版)","og:type":"book","og:url":"https://zuji.eu.org/books/36632126/Data-Aggregation-and-Group-Operations","og:image":"https://i-blog.csdnimg.cn/direct/a3631c7292b546cc8982429c96df4bb4.png","og:image:width":"50","og:image:alt":"bookcover"},"tags":["program/python"],"dgShowInlineTitle":true,"created":"2025-09-16 07:00","updated":"2025-09-21 18:10"}
---

# 10.1 GroupBy 机制

Hadley Wickham（R 语言中许多热门包的作者）创造了一个用于描述分组运算的术语“拆分- 应用- 联合”（split- apply- combine）。首先，pandas 对象（无论是 Series、DataFrame 还是其他）中的数据会根据你所提供的单键或多键拆分为多组。拆分操作是在对象的特定轴上执行的。例如，DataFrame 可以在其行（axis="index"）或列（axis="columns"）上进行分组。其次，将一个函数应用到各个分组，并产生一个新值。最后，所有这些函数的执行结果会联合为最终的结果对象。结果对象的形式一般取决于对数据所执行的操作。图 10- 1 对简单的聚合过程做了大致说明。

![](https://cdn-mineru.openxlab.org.cn/result/2025-09-15/af04d75d-9613-4dcf-a3e0-cf9b482a4413/926a395b21cca16208dd9c3c0f60541b68a1c14bcb35135ee2646dbb5df0b9ce.jpg)  
图 10-1：分组聚合图示

每个分组键可以有多种形式，并且类型不必相同：

·值列表或值数组，其长度与待分组的轴一样。

·在 DataFrame 中指明列名的值。

·字典或 Series，给出待分组轴上的值与分组名之间的对应关系。

·可以在轴索引或索引各标签上调用的函数。

注意，后三种都是快捷方式，用于产生一组用于拆分对象的值数组。如果觉得它们看起来很抽象，也不用担心，本章将给出与这些方法有关的大量示例。首先来看下面这个 DataFrame——一个非常简单的表格型数据集：

```
df = pd.DataFrame ({"key 1": ["a", "a", None, "b", "b", "a", None], "key 2": pd.Series ([1, 2, 1, 2, 1, None, 1], dtype="I nt 64"), "data 1": np. random. standard_normal (7), "data 2": np. random. standard_normal (7)})

df Out[15]: key 1 key 2 data 1 data 2 0 a 1 - 0.204798 0.281746 1 a 2 0.478943 0.769023 2 None 1 - 0.519439 1.246435 3 b 2 - 0.555730 1.007189 4 b 1 1.965781 - 1.296221 5 a <NA> 1.393406 0.274992 6 None 1 0.092908 0.228913
```

假设你想要按 key 1 标签进行分组，并计算 data 1 列的平均值。有多种方法可以实现，其中一种方法是访问 data 1，并使用 key 1 列（这是一个 Series）调用 groupby 方法：

```
grouped = df["data 1"]. groupby (df["key 1"])  grouped  Out[17]: <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fa9270e0a00>
```

变量 grouped 是一个特殊的 GroupBy 对象。它实际上还没有进行任何计算，只是包含一些有关分组键 df["key 1"]的中间数据而已。换句话说，该对象已经有了接下来对各分组执行运算所需的一切信息。例如，我们可以调用 GroupBy 对象的 mean 方法来计算分组平均值：

grouped.mean ()  Out[18]: key 1  a 0.555881  b 0.705025  Name: data 1, dtype: float 64

10.2 节将详细讲解.mean () 的调用过程。这里最重要的是，通过分组键对数据数据（Series）进行分割，然后做了聚合，并生成了一个新 Series，其索引为 key 1 列中的唯一值。之所以结果中索引的名称为 key 1，是因为原始 DataFrame 的 df["key 1"]的列名就是如此。

如果我们一次传入包含多个数组的列表，就会得到不同的结果：

means = df["data 1"]. groupby ([df["key 1"], df["key 2"]]). mean ()

meansOut[20]: key 1 key 2 a 1 - 0.2047082 0.478943 b 1 1.9657812 - 0.555730 Name: data 1, dtype: float 64

这里，我通过两个键对数据进行了分组，得到的 Series 具有一个层次化索引，由唯一的键对组成：

means.unstack () Out[21]: key 2 1 2 key 1 a - 0.204708 0.478943 b 1.965781 - 0.555730

在这个例子中，分组键均为 Series。实际上，分组键可以是长度正确的任意数组：

states = np.array (["OH", "CA", "CA", "OH", "OH", "CA", "OH"]) years = [2005, 2005, 2006, 2005, 2006, 2005, 2006]df["data 1"]. groupby ([states, years]). mean () Out[24]: CA 2005 0.9361752006 - 0.519439 OH 2005 - 0.3802192006 1.029344 Name: data 1, dtype: float 64

对于要处理的 DataFrame，分组信息往往就位于其中。在这种情况下，你可以将列名（无论是字符串、数字还是其他 Python 对象）用作分组键：

df.groupby ("key 1"). mean () Out[25]: key 2 data 1 data 2

key 1

a 1.5 0.555881 0.441920 b 1.5 0.705025 - 0.144516

df.groupby ("key 2"). mean ()

Out[26]:

data 1 data 2

key 2

1 0.333636 0.115218

2 - 0.038393 0.888106

df. groupby["key 1", "key 2"]). mean ()

Out[27]:

data 1 data 2 key 1 key 2 a 1 - 0.204708 0.281746 2 0.478943 0.769023 b 1 1.965781 - 1.296221 2 - 0.555730 1.007189

你可能已经注意到了，第二个例子在执行 df. groupby（"key 1"）. mean () 时，结果中没有 key 1 列。这是因为 df"key 1"不是数值数据，是个冗余列，所以从结果中移除了。默认情况下，所有数值列都会被聚合，虽然有时可能会过滤到子集中，稍后就会碰到这种情况。

无论使用 groupby 的目的是什么，GroupBy 对象的 size 方法都很实用，它可以返回一个含有分组大小的 Series：

df. groupby["key 1", "key 2"]). size ()

Out[28]:

key 1 key 2

a 1 1

2 1

b 1 1

2 1

dtype: int 64

注意，任何分组关键字中的缺失值，默认都会从结果中除去。向 groupby 中传入 dropna=False 可以禁用该功能：

df.groupby ("key 1", dropna=False). size () Out[29]: key 1 a 3 b 2 NaN 2 dtype: int 64

df. groupby["key 1", "key 2"], dropna=False). size () Out[30]: key 1 key 2 a 1 1 2 1 <NA> 1 b 1 1 2 1 NaN 1 2 dtype: int 64

在分组函数中，与 size 具有一定相似性的是 count 方法，它可以计算每个分组中非空值的数量：

df.groupby ("key 1"). count () Out[31]: key 2 data 1 data 2 key 1 a 2 3 3 b 2 2 2

# 10.1.1 对分组进行选代

groupby 返回的对象支持选代，可以产生一个二元组构成的序列，每个元组包含分组名和数据块。看下面的例子：

for name, group in df.groupby ("key 1"): print (name) print (group) a key 1 key 2 data 1 data 2 0 a 1 - 0.204708 0.281746 1 a 2 0.478943 0.769023 5 a <NA> 1.393406 0.274992 b key 1 key 2 data 1 data 2 3 b 2 - 0.555730 1.007189 4 b 1 1.965781 - 1.296221

对于具有多个分组键的情况，元组的第一个元素是由键值组成的元组：

for (k 1, k 2), group in df.groupby (["key 1", "key 2"]:    ...: print ((k 1, k 2))    ...: print (group)    ...: ('a', 1)    key 1 key 2 data 1 data 2    0 a 1 - 0.204708 0.281746    ('a', 2)    key 1 key 2 data 1 data 2    1 a 2 0.478943 0.769023    ('b', 1)    key 1 key 2 data 1 data 2    4 b 1 1.965781 - 1.296221    ('b', 2)    key 1 key 2 data 1 data 2    3 b 2 - 0.55573 1.007189 当然，你可以对这些数据块做任何操作。你可能会觉得这个运算很有用，用一行代码计算得到一个包含这些数据块的字典：

当然，你可以对这些数据块做任何操作。你可能会觉得这个运算很有用，用一行代码计算得到一个包含这些数据块的字典：

pieces = {name: group for name, group in df.groupby ("key 1")}pieces["b"]Out[35]:    key 1 key 2 data 1 data 2    3 b 2 - 0.555730 1.007189    4 b 1 1.965781 - 1.296221

groupby 默认是在 axis="index"上进行分组的，通过设置也可以在任意其他轴上进行分组。还是以 df 为例，我们可以根据其列是以"key"还是"data"开头进行分组：

grouped = df.groupby ({"key 1": "key", "key 2": "key", "data 1": "data", "data 2": "data"}, axis="columns")

可以如下打印分组：

for group_key, group_values in grouped: print (group_key) print (group_values) data data 1 data 2 0- 0.204708 0.281746 1 0.478943 0.769023 2 - 0.519439 1.246435 3 - 0.555730 1.007189 4 1.965781 - 1.296221 5 1.393406 0.274992 6 0.092908 0.228913 key key 1 key 2 0 a 1 1 a 2 2 None 1 3 b 2 4 b 1 5 a <NA> 6 None 1

# 10.1.2 选取一列或多列

使用单个列名或列名数组对由 DataFrame 创建的 GroupBy 对象建立索引，能实现选取部分列并进行聚合的效果。这表明：

df.groupby ("key 1")["data 1"] df.groupby ("key 1") [["data2"\|"data2"]]

是以下代码的语法糖：

df["data 1"]. groupby (df["key 1"]) df [['data2'\|'data2']]. groupby (df["key 1"])

尤其是对于大型数据集，可能只想对部分列进行聚合。例如，对于前面的数据集，如果只需计算 data 2 列的平均值并以 DataFrame 形式得到结果，可以这样写：

df.groupby (['key 1", "key 2']) [['data2'\|'data2']]. mean () Out[38]: data 2 key 1 key 2 a 1 0.281746 2 0.769023 b 1 - 1.296221 2 1.007189

如果传入的是列表或数组，则该索引操作所返回的对象是一个经过分组的 DataFrame。如果传入的是标量形式的单个列名，则返回的对象为经过分组的 Series：

s_grouped  $=$  df. groupby（["key 1"，"key 2"]）["data 2"] s_grouped Out[40]: <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fa9270e3520> s_grouped.mean () Out[41]: key 1 key 2 a 1 0.281746 2 0.769023 b 1 - 1.296221 2 1.007189 Name: data 2，dtype: float 64

# 10.1.3 利用字典和 Series 进行分组

除了数组，分组信息还能以其他形式存在。来看另一个示例 DataFrame：

people  $=$  pd.DataFrame (np. random. standard_normal ((5，5)), columns  $=$  ["a"，"b"，"c"，"d"，"e"]，index  $=$  ["Joe"，"Steve"，"Wanda"，"Jill"，"Trey"]) people. iloc[2:3，[1，2]]  $=$  np. nan #加入一些空值peopleOut[44]: a b C d eJoe 1.352917 0.886429 - 2.001637 - 0.371843 1.669025 Steve - 0.438570 - 0.539741 0.476985 3.248944 - 1.021228 Wanda - 0.577087 NaN NaN 0.523772 0.000940 Jill 1.343810 - 0.713544 - 0.831154 - 2.370232 - 1.860761 Trey - 0.860757 0.566145 - 1.265934 0.119827 - 1.063512

现在，假设已知列的分组关系，并希望根据分组计算列的和：

mapping  $=$  {"a"："red"，"b"："red"，"c"："blue"， "d"："blue"，"e"："red"，"f"："orange"}

现在，可以将这个字典传给 groupby 来构造数组，但我们可以直接传递字典（这里包含了键"f"，用于强调即使存在未使用的分组键也是可以的）：

by_column = people.groupby (mapping, axis="columns")

by_column.sum () Out[47]:

blue red Joe - 2.373480 3.968371 Steve 3.725929 - 1.999539 Wanda 0.523772 - 0.576147 Jill - 3.201385 - 1.230495 Trey - 1.146107 - 1.364125

Series 也有同样的功能，可以看作固定大小的映射：

map_series = pd.Series (mapping)

map_series Out[49]:

a red b red c blue d blue e red f orange dtype: object

people.groupby (map_series, axis="columns"). count () Out[50]:

blue red Joe 2 3 Steve 2 3 Wanda 1 2 Jill 2 3 Trey 2 3

# 10.1.4 利用函数进行分组

与使用字典或 Series 相比，使用 Python 函数是一种更加通用的定义分组映射的方式。任何作为分组键的函数都会在各个索引值上调用一次（如果使用 axis="columns"，则在各个列上调用一次函数），其返回值会用作分组名称。具体地讲，以上一小节的示例 DataFrame 为例，其索引值为人的名字。假设你想根据人名长度分组。虽然可以计算一个字符串长度的数组，但更简单的方法是传入 len 函数：

people.groupby (len). sum () Out[51]: a b C d e 3 1.352917 0.886429 - 2.001637 - 0.371843 1.669025 4 0.483052 - 0.153399 - 2.097088 - 2.250405 - 2.924273 5 - 1.015657 - 0.539741 0.476985 3.772716 - 1.020287

将函数与数组、列表、字典、Series 混合使用也不是问题，因为它们在内部都会转换为数组：

key_list = ["one", "one", "one", "two", "two"]  people.groupby ([len, key_list]). min ()  Out[53]:  a b c d e  3 one 1.352917 0.886429 - 2.001637 - 0.371843 1.669025  4 two - 0.860757 - 0.713544 - 1.265934 - 2.370232 - 1.860761  5 one - 0.577087 - 0.539741 0.476985 0.523772 - 1.021228

# 10.1.5 根据索引层级分组

层次化索引数据集最方便的地方，就在于它能够根据轴索引的某个层级进行聚合。来看一个示例：

columns  $=$  pd. MultiIndex. from_arrays ([["US","US","US","JP","JP"],[1,3,5,1,3\|"US","US","US","JP","JP"],[1,3,5,1,3]], names  $=$  ["cty"，"tenor"]) hier_df  $=$  pd.DataFrame (np. random. standard_normal ((4,5)), columns  $\equiv$  columnS) hier_dfOut[56]: cty US JPtenor 1 3 5 1 30.332883 - 2.359419 - 0.199543 - 1.541996 - 0.9707361 - 1.307030 0.286350 0.377984 - 0.753887 0.33128621.349742 0.069877 0.246674 - 0.011862 1.0048123.327195 - 0.919262 - 1.549106 0.022185 0.758363

要根据层级分组，可以将层级数值或名称传递给 level 关键字：

hier_df.groupby (level="cty", axis="columns"). count ()  Out[57]:  cty JP US  0 2 3  1 2 3  2 2 3  3 2 3

# 10.2 数据聚合

聚合指的是能够从数组产生标量值的任何数据转换过程。之前的例子使用过一些聚合运算，包括 mean、count、min 和 sum。你可能想知道在 GroupBy 对象上调用 mean () 时究竟发生了什么。许多常见的聚合运算（如表 10- 1 所示）都对实现做了优化。当然，除了这些方法，你还可以使用其他聚合方法。

表 10- 1：经过优化的 groupby 方法

<table><tr><td>函数名</td><td>说明</td></tr><tr><td>any、all</td><td>如果存在非空值或全部为非 NA 值，则返回 True</td></tr><tr><td>count</td><td>非 NA 值的数量</td></tr><tr><td>cumin、cummax</td><td>非 NA 值的累计最小值和最大值</td></tr><tr><td>cumsum</td><td>非 NA 值的累计和</td></tr><tr><td>cumprod</td><td>非 NA 值的累计乘积</td></tr><tr><td>first、last</td><td>第一个非 NA 值和最后一个非 NA 值</td></tr><tr><td>mean</td><td>非 NA 值的平均值</td></tr><tr><td>median</td><td>非 NA 值的算术中位数</td></tr><tr><td>min、max</td><td>非 NA 值的最小值和最大值</td></tr><tr><td>nth</td><td>若数据为有序排列时，返回位置 n 处的值</td></tr><tr><td>ohlc</td><td>对于时间序列类型数据，计算“开-高-低-收”四个统计值</td></tr><tr><td>prod</td><td>非 NA 值的乘积</td></tr><tr><td>quantile</td><td>计算样本分位数</td></tr><tr><td>rank</td><td>非 NA 值的顺序排列，比如调用 Series. rank</td></tr><tr><td>size</td><td>计算分组大小，返回的结果为 Series</td></tr><tr><td>sum</td><td>非 NA 值的和</td></tr><tr><td>std、var</td><td>样本标准差和方差</td></tr></table>

你可以使用自己定制的聚合算法，还可以调用分组对象上已经定义好的任何方法。例如，Series 的 nsmallest 方法可以计算数据中的若干最小值。虽然 nsmallest 并不是特意为 GroupBy 实现的，没有经过实现优化，但仍然可以用于 GroupBy 对象。由表及里，GroupBy 实际上是先对 Series 进行切片，然后对各片调用 piece. nsmallest（n），最后将这些结果组装成最终结果：

In[58]: df Out[58]: key 1 key 2 data 1 data 2 0 a 1 - 0.204708 0.281746 1 a 2 0.478943 0.769023 2 None 1 - 0.519439 1.246435 3 b 2 - 0.555730 1.007189 4 b 1 1.965781 - 1.296221 5 a <NA> 1.393406 0.274992 6 None 1 0.092908 0.228913

In[59]: grouped = df.groupby ("key 1") In[60]: grouped["data 1"]. nsmallest (2)

Out[60]: key 1 a 0 - 0.204708 1 0.478943 b 3 - 0.555730 4 1.965781 Name: data 1, dtype: float 64

要想使用自己的聚合函数，只需将能聚合数组的函数传入 aggregate（或其缩写 agg）方法：

def peak_to_peak (arr):...: return arr.max () - arr.min () grouped.agg (peak_to_peak) Out[62]: key 2 data 1 data 2 key 1 a 1 1.598113 0.494031 b 1 2.521511 2.303410

你可能注意到，某些方法（比如 describe）也可以用于 GroupBy，严格来讲，它们并非聚合运算：

grouped.describe () Out[63]: key 2 data 1 count mean std min 25% 50% 75% max count mean key 1 a 2.0 1.5 0.707107 1.0 1.25 1.5 1.75 2.0 3.0 0.555881 b 2.0 1.5 0.707107 1.0 1.25 1.5 1.75 2.0 2.0 0.705025 data 2 75% max count mean std min 25% key 1 a 0.936175 1.393406 3.0 0.441920 0.283299 0.274992 0.278369 b 1.335403 1.965781 2.0 - 0.144516 1.628757 - 1.296221 - 0.720368 50% 75% max key 1 a 0.281746 0.525384 0.769023 b - 0.144516 0.431337 1.007189 [2 rows x 24 columns]

其中具体发生了什么，我将在 10.3 节中讲解。

自定义的聚合函数通常要比表 10- 1 中经过优化的函数慢得多。这是因为在构造中间分组数据块时存在非常大的开销（函数调用、数据重排）。

# 10.2.1 逐列操作和多函数应用

回到上一章的小费数据集。使用 pandas. read_csv 加载数据后，我们添加一个小费百分比的列：

tips = pd. read_csv ("examples/tips. csv")

tips.head () Out[65]:

total_bill tip smoker day time size 0 16.99 1.01 No Sun Dinner 2 1 10.34 1.66 No Sun Dinner 3 2 21.01 3.50 No Sun Dinner 3 3 23.68 3.31 No Sun Dinner 2 4 24.59 3.61 No Sun Dinner 4

现在，用小费占总账单的百分比新增一个列 tip_pct:

tips["tip_pct"] = tips["tip"] / tips["total_bill"]

tips.head () Out[67]:

total_bill tip smoker day time size tip_pct 0 16.99 1.01 No Sun Dinner 2 0.059447 1 10.34 1.66 No Sun Dinner 3 0.160542 2 21.01 3.50 No Sun Dinner 3 0.166587 3 23.68 3.31 No Sun Dinner 2 0.139780 4 24.59 3.61 No Sun Dinner 4 0.146808

你已经看到，对 Series 或 DataFrame 所有列的聚合运算其实就是使用 aggregate（或 agg）配合所需的函数，或者调用 mean、std 之类的方法。然而，你可能希望对不同的列使用不同的聚合函数，或一次应用多个函数。其实这也不难实现，我将通过一些例子进行示范。首先，我根据 day 和 smoker 对 tips 进行分组：

grouped = tips.groupby (["day", "smoker"])

注意，对于表 10- 1 中的描述性统计，可以将函数名以字符串的形式传入：

grouped_pct = grouped["tip_pct"]

grouped_pct.agg ("mean") Out[70]:

day smoker Fri No 0.151650 Yes 0.174783 Sat No 0.158948 Yes 0.147906 Sun No 0.160113 Yes 0.187250 Thur No 0.160298 Yes 0.163863 Name: tip_pct, dtype: float 64

如果传入的是函数或函数名的列表，得到的 DataFrame 的列就会以相应的函数命名：

grouped_pct.agg (["mean", "std", peak_to_peak]) Out[71]: mean std peak_to_peak day smoker Fri No 0.151650 0.028123 0.067349 Yes 0.174783 0.051293 0.159925 Sat No 0.158048 0.039767 0.235193 Yes 0.147906 0.061375 0.290095 Sun No 0.160113 0.042347 0.193226

这里，我们给 agg 传递了一个聚合函数的列表，这些函数会各自对数据分组进行评估。

你并非一定要接收 GroupBy 给出的那些列名。特别是 lambda 函数，它的名称是"，辨识度很低（可以查看函数的__name__属性）。因此，如果传入的是由（name，function）元组构成的列表，则各元组的第一个元素就会用作 DataFrame 的列名（可以将这种二元组列表当作有序映射）：

grouped_pct.agg ([("average", "mean"), ("stdev", np. std)]) Out[72]:

average stdev day smoker Fri No 0.151650 0.028123 Yes 0.174783 0.051293 Sat No 0.158048 0.039767 Yes 0.147906 0.061375 Sun No 0.160113 0.042347 Yes 0.187250 0.154134 Thur No 0.160298 0.038774 Yes 0.163863 0.039389

对于 DataFrame，你还有更多选择。你可以定义一个函数列表，应用到所有列，或者不同的列应用不同的函数。假设我们想要对 tip_pct 和 total_bill 列计算得到 3 个相同的统计值：

functions = ["count", "mean", "max"]

result = grouped (["tip_pct", "total_bill)]. agg (functions)

result Out[75]:

<table><tr><td></td><td></td><td colspan="2">tip_pct</td><td colspan="4">total_bill</td></tr><tr><td></td><td></td><td>count</td><td>mean</td><td>max</td><td>count</td><td>mean</td><td>max</td></tr><tr><td>day</td><td>smoker</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">Fri</td><td>No</td><td>4</td><td>0.151650</td><td>0.187735</td><td>4</td><td>18.420000</td><td>22.75</td></tr><tr><td>Yes</td><td>15</td><td>0.174783</td><td>0.263480</td><td>15</td><td>16.813333</td><td>40.17</td></tr><tr><td rowspan="2">Sat</td><td>No</td><td>45</td><td>0.158048</td><td>0.291990</td><td>45</td><td>19.661778</td><td>48.33</td></tr><tr><td>Yes</td><td>42</td><td>0.147906</td><td>0.325733</td><td>42</td><td>21.276667</td><td>50.81</td></tr><tr><td>Sun</td><td>No</td><td>57</td><td>0.160113</td><td>0.252672</td><td>57</td><td>20.506667</td><td>48.17</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>Yes</td><td>19</td><td>0.187250</td><td>0.710345</td><td>19</td><td>24.120000</td><td>45.35</td></tr><tr><td rowspan="2">Thur</td><td>No</td><td>45</td><td>0.160298</td><td>0.266312</td><td>45</td><td>17.113111</td><td>41.19</td></tr><tr><td>Yes</td><td>17</td><td>0.163863</td><td>0.241255</td><td>17</td><td>19.190588</td><td>43.11</td></tr></table>

如你所见，产生的 DataFrame 具有层次化的列，这相当于分别对各列进行聚合，然后将列名作为 keys 参数，并用 concat 方法将结果拼接到一起：

<table><tr><td colspan="4">result[&quot; tip_pct&quot;]</td></tr><tr><td colspan="4">Out[76]:</td></tr><tr><td></td><td>count</td><td>mean</td><td>max</td></tr><tr><td>day</td><td>smoker</td><td></td><td></td></tr><tr><td rowspan="2">Fri</td><td>No</td><td>4</td><td>0.151650</td></tr><tr><td>Yes</td><td>15</td><td>0.174783</td></tr><tr><td rowspan="2">Sat</td><td>No</td><td>45</td><td>0.158048</td></tr><tr><td>Yes</td><td>42</td><td>0.147906</td></tr><tr><td rowspan="2">Sun</td><td>No</td><td>57</td><td>0.160113</td></tr><tr><td>Yes</td><td>19</td><td>0.187250</td></tr><tr><td rowspan="2">Thur</td><td>No</td><td>45</td><td>0.160298</td></tr><tr><td>Yes</td><td>17</td><td>0.163863</td></tr></table>

与之前一样，这里也可以传入带有自定义名称的元组列表：

ftuples = ["Average", "mean"), ("Variance", np. var)]

grouped["tip_pct", "total_bill"]. agg (ftuples) Out[78]:

<table><tr><td></td><td></td><td>tip_pct</td><td colspan="3">total_bill</td></tr><tr><td></td><td></td><td>Average</td><td>Variance</td><td>Average</td><td>Variance</td></tr><tr><td>day</td><td>smoker</td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">Fri</td><td>No</td><td>0.151650</td><td>0.000791</td><td>18.420000</td><td>25.596333</td></tr><tr><td>Yes</td><td>0.174783</td><td>0.002631</td><td>16.813333</td><td>82.562438</td></tr><tr><td rowspan="2">Sat</td><td>No</td><td>0.158048</td><td>0.001581</td><td>19.661778</td><td>79.908965</td></tr><tr><td>Yes</td><td>0.147906</td><td>0.003767</td><td>21.276667</td><td>101.387535</td></tr><tr><td rowspan="2">Sun</td><td>No</td><td>0.160113</td><td>0.001793</td><td>20.506667</td><td>66.099980</td></tr><tr><td>Yes</td><td>0.187250</td><td>0.023757</td><td>24.120000</td><td>109.046044</td></tr><tr><td rowspan="2">Thur</td><td>No</td><td>0.160298</td><td>0.001503</td><td>17.113111</td><td>59.625081</td></tr><tr><td>Yes</td><td>0.163863</td><td>0.001551</td><td>19.190588</td><td>69.808518</td></tr></table>

现在，假设你想要对单列或多列应用不同的函数。具体的办法是向 agg 传入一个从列名映射到函数规范的字典：

grouped.agg ({"tip" : np. max, "size" : "sum"}) Out[79]:

tip size day smoker Fri No 3.50 9 Yes 4.73 31 Sat No 9.00 115 Yes 10.00 104 Sun No 6.00 167 Yes 6.50 49 Thur No 6.70 112

Yes 5.00 40

grouped.agg ({"tip_pct" : ["min", "max", "mean", "std"], ...: "size" : "sum"}) Out[80]:

Out[80]:

<table><tr><td rowspan="2"></td><td colspan="3">tip_pct</td><td colspan="2">size</td><td></td></tr><tr><td>min</td><td>max</td><td>mean</td><td>std</td><td>sum</td><td></td></tr><tr><td>day</td><td>smoker</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">Fri</td><td>No</td><td>0.120385</td><td>0.187735</td><td>0.151650</td><td>0.028123</td><td>9</td></tr><tr><td>Yes</td><td>0.103555</td><td>0.263480</td><td>0.174783</td><td>0.051293</td><td>31</td></tr><tr><td rowspan="2">Sat</td><td>No</td><td>0.056797</td><td>0.291990</td><td>0.158048</td><td>0.039767</td><td>115</td></tr><tr><td>Yes</td><td>0.035638</td><td>0.325733</td><td>0.147906</td><td>0.061375</td><td>164</td></tr><tr><td rowspan="2">Sun</td><td>No</td><td>0.059447</td><td>0.252672</td><td>0.160113</td><td>0.042347</td><td>167</td></tr><tr><td>Yes</td><td>0.065660</td><td>0.710345</td><td>0.187250</td><td>0.154134</td><td>49</td></tr><tr><td rowspan="2">Thur</td><td>No</td><td>0.072961</td><td>0.266312</td><td>0.160298</td><td>0.038774</td><td>112</td></tr><tr><td>Yes</td><td>0.090014</td><td>0.241255</td><td>0.163863</td><td>0.039389</td><td>40</td></tr></table>

只有将多个函数应用到至少一列时，DataFrame 才会具有层次化的列。

# 10.2.2 返回不含行索引的聚合数据

对于到目前为止的所有示例，返回的聚合数据都具有索引（可能还是层次化索引），索引由唯一的用于分组的键组合构成。由于并不总是需要索引，因此你可以向 groupby 传入 as_index=False 以禁用该功能：

tips.groupby (['day", "smoker"], as_index=False). mean () Out[81]: day smoker total_bill tip size tip_pct 0 Fri No 18.420000 2.812500 2.250000 0.151650 1 Fri Yes 16.813333 2.714000 2.066667 0.174783 2 Sat No 19.661778 3.102889 2.555556 0.158048 3 Sat Yes 21.276667 2.875476 2.476190 0.147906 4 Sun No 20.506667 3.167895 2.929825 0.160113 5 Sun Yes 24.120000 3.516842 2.578947 0.187250 6 Thur No 17.113111 2.673778 2.488889 0.160298 7 Thur Yes 19.190588 3.030000 2.352941 0.163863

当然，对结果调用 reset_index 也能得到这种形式的结果。使用 as_index=False 参数可以避免一些不必要的计算。

# 10.3 Apply：通用的“拆分-应用-联合”范式

GroupBy 方法中最通用的是 apply 方法，它是本节的重点。apply 会将待处理的对象拆分成多个片段，然后对各片段调用传入的函数，最后尝试将各片段拼接到一起。

回到之前的小费数据集，假设你想要根据分组选出最高的 5 个 tip_pct 值。首先，编写一个可以在特定列中选取最大值所在行的函数：

def top (df, n=5, column="tip_pct"):    ...: return df. sort_values (column, ascending=False)[: n]

top (tips, n=6) Out[83]:

total_bill tip_smoker day time size tip_pct 172 7.25 5.15 Yes Sun Dinner 2 0.710345 178 9.60 4.00 Yes Sun Dinner 2 0.416667 67 3.07 1.00 Yes Sat Dinner 1 0.325733 232 11.61 3.39 No Sat Dinner 2 0.291990 183 23.17 6.50 Yes Sun Dinner 4 0.280535 109 14.31 4.00 Yes Sat Dinner 2 0.279525

现在，如果按照 smoker 分组，随后调用 apply 以及该函数，就会得到：

tips.groupby ("smoker"). apply (top) Out[84]:

<table><tr><td></td><td>total_bill</td><td>tip smoker</td><td>day</td><td>time</td><td>size</td><td>tip_pct</td></tr><tr><td>smoker</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="5">No</td><td>232</td><td>11.61</td><td>3.39</td><td>No</td><td>Sat</td><td>Dinner</td></tr><tr><td>149</td><td>7.51</td><td>2.00</td><td>No</td><td>Thur</td><td>Lunch</td></tr><tr><td>51</td><td>10.29</td><td>2.60</td><td>No</td><td>Sun</td><td>Dinner</td></tr><tr><td>185</td><td>20.69</td><td>5.00</td><td>No</td><td>Sun</td><td>Dinner</td></tr><tr><td>88</td><td>24.71</td><td>5.85</td><td>No</td><td>Thur</td><td>Lunch</td></tr><tr><td rowspan="5">Yes</td><td>172</td><td>7.25</td><td>5.15</td><td>Yes</td><td>Sun</td><td>Dinner</td></tr><tr><td>178</td><td>9.69</td><td>4.00</td><td>Yes</td><td>Sun</td><td>Dinner</td></tr><tr><td>67</td><td>3.07</td><td>1.00</td><td>Yes</td><td>Sat</td><td>Dinner</td></tr><tr><td>183</td><td>23.17</td><td>6.50</td><td>Yes</td><td>Sun</td><td>Dinner</td></tr><tr><td>109</td><td>14.31</td><td>4.00</td><td>Yes</td><td>Sat</td><td>Dinner</td></tr></table>

这里发生了什么？首先，基于 smoker 的值，将 tips DataFrame 进行分组。然后在各个分组上调用函数 top，结果由 pandas. concat 拼装到一起，并使用分组名作为各组的标签。于是，最终结果就有了一个层次化索引，其内层索引包含原 DataFrame 的索引值。

如果让传给 apply 的函数还接收其他参数或关键字，则可以将这些内容放在函数名后面一并传入：

tips. groupby["smoker", "day"]). apply (top, n=1, column="total_bill") Out[85]:

<table><tr><td></td><td>total_bill</td><td>tip smoker</td><td>day</td><td>time</td><td>size</td><td>tip_pct</td></tr><tr><td>smoker day</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="4">No</td><td>Fri 94</td><td>22.75</td><td>3.25</td><td>No</td><td>Fri</td><td>Dinner</td></tr><tr><td>Sat 212</td><td>48.33</td><td>9.00</td><td>No</td><td>Sat</td><td>Dinner</td></tr><tr><td>Sun 156</td><td>48.17</td><td>5.00</td><td>No</td><td>Sun</td><td>Dinner</td></tr><tr><td>Thur 142</td><td>41.19</td><td>5.00</td><td>No</td><td>Thur</td><td>Lunch</td></tr><tr><td rowspan="4">Yes</td><td>Fri 95</td><td>40.17</td><td>4.73</td><td>Yes</td><td>Fri</td><td>Dinner</td></tr><tr><td>Sat 170</td><td>50.81</td><td>10.00</td><td>Yes</td><td>Sat</td><td>Dinner</td></tr><tr><td>Sun 182</td><td>45.35</td><td>3.50</td><td>Yes</td><td>Sun</td><td>Dinner</td></tr><tr><td>Thur 197</td><td>43.11</td><td>5.00</td><td>Yes</td><td>Thur</td><td>Lunch</td></tr></table>

除了以上这些基本用法，要充分利用 apply 还需要一些创新性。使用者掌控着传入函数的内部细节，该函数必须返回一个 pandas 对象或一个标量值。本章剩余部分主要是用示例展示如何利用 groupby 解决各种问题。

例如，可能你已经回想起来了，之前我在 GroupBy 对象上调用过 describe：

result = tips.groupby ("smoker")["tip_pct"]. describe ()

result Out[87]:

count mean std min 25% 50% 75% \smoker No 151.0 0.159328 0.039910 0.056797 0.136906 0.155625 0.185014 Yes 93.0 0.163196 0.085119 0.035638 0.106771 0.153846 0.195059

max smoker No 0.291990 Yes 0.710345

result.unstack ("smoker") Out[88]:

smoker count No 151.000000 Yes 93.000000 mean No 0.159328 Yes 0.163196 std No 0.039910 Yes 0.085119 min No 0.056797 Yes 0.035638 25% No 0.136906 Yes 0.106771 50% No 0.155625 Yes 0.153846 75% No 0.185014 Yes 0.195059 max No 0.291990 Yes 0.710345 dtype: float 64

在 GroupBy 内部，当调用 describe 之类的方法时，实际上是以快捷方式的形式实现了以下代码：

def f (group):    return group.describe () grouped.apply (f)

# 10.3.1 禁用分组键

从上面的例子中可以看出，分组键会与原始对象各分块的索引共同构成结果对象中的层次化索引。将 group_keys=False 传入 groupby 即可禁止该效果：

tips.groupby ("smoker", group_keys=False). apply (top) Out[89]:

<table><tr><td></td><td>total_bill</td><td>tip smoker</td><td>day</td><td>time</td><td>size</td><td>tip_pct</td></tr><tr><td>232</td><td>11.61</td><td>3.39</td><td>No</td><td>Sat</td><td>Dinner</td><td>2 0.291990</td></tr><tr><td>149</td><td>7.51</td><td>2.00</td><td>No</td><td>Thur</td><td>Lunch</td><td>2 0.266312</td></tr><tr><td>51</td><td>10.29</td><td>2.60</td><td>No</td><td>Sun</td><td>Dinner</td><td>2 0.252672</td></tr><tr><td>185</td><td>20.69</td><td>5.00</td><td>No</td><td>Sun</td><td>Dinner</td><td>5 0.241663</td></tr><tr><td>88</td><td>24.71</td><td>5.85</td><td>No</td><td>Thur</td><td>Lunch</td><td>2 0.236746</td></tr><tr><td>172</td><td>7.25</td><td>5.15</td><td>Yes</td><td>Sun</td><td>Dinner</td><td>2 0.710345</td></tr><tr><td>178</td><td>9.60</td><td>4.00</td><td>Yes</td><td>Sun</td><td>Dinner</td><td>2 0.416667</td></tr><tr><td>67</td><td>3.07</td><td>1.00</td><td>Yes</td><td>Sat</td><td>Dinner</td><td>1 0.325733</td></tr><tr><td>183</td><td>23.17</td><td>6.50</td><td>Yes</td><td>Sun</td><td>Dinner</td><td>4 0.280535</td></tr><tr><td>109</td><td>14.31</td><td>4.00</td><td>Yes</td><td>Sat</td><td>Dinner</td><td>2 0.279525</td></tr></table>

# 10.3.2 分位数和桶分析

第 8 章曾讲过，pandas 有一些能根据指定分箱或样本分位数将数据进行分桶的工具，特别是 pandas. cut 和 pandas. qcut。将这些函数与 groupby 结合起来，能非常轻松地实现对数据集的桶分析或分位数分析。考虑下面这个简单的随机数据集，以及使用 pandas. cut 得到的等长桶分类：

frame = pd.DataFrame ({"data 1": np. random. standard_normal (1000), "data 2": np. random. standard_normal (1000)})

frame.head () Out[91]:

data 1 data 2 0- 0.660524 - 0.612905 1 0.862580 0.316447 2 - 0.010032 0.838295 3 0.050009 - 1.034423 4 0.670216 0.434304

quartiles = pd.cut (frame["data 1"], 4)

quartiles.head (10)

Out[93]:

0 (- 1.23, 0.489] 1 (0.489, 2.208] 2 (- 1.23, 0.489] 3 (- 1.23, 0.489] 4 (0.489, 2.208] 5 (0.489, 2.208] 6 (- 1.23, 0.489] 7 (- 1.23, 0.489] 8 (- 2.956, - 1.23] 9 (- 1.23, 0.489] Name: data 1, dtype: category

Categories (4, interval[float 64, right]): [(- 2.956, - 1.23] < (- 1.23, 0.489] < (0.489, 2.208] < (2.208, 3.928)]

由 cut 返回的 Categorical 对象可直接传递给 groupby。因此，我们可以如下计算分位数的分组统计集合：

def get_stats (group):...: return pd.DataFrame ({min": group.min ()，"max": group.max ()，"count": group.count ()，"mean": group.mean ()}

grouped = frame.groupby (quartiles)

grouped.apply (get_stats) Out[96]:

min max count mean data 1 - 2.956，- 1.23] data 1 - 2.949343 - 1.230179 94 - 1.658818 data 2 - 3.399312 1.670835 94 - 0.033333 (- 1.23，0.489] data 1 - 1.228918 0.488675 598 - 0.329524 data 2 - 2.989741 3.260383 598 - 0.002622 (0.489，2.208] data 1 0.489965 2.200997 298 1.065727 data 2 - 3.745356 2.954439 298 0.078249 (2.208，3.928] data 1 2.212303 3.927528 10 2.644253 data 2 - 1.929776 1.765640 10 0.024750

使用下面的方法可以更简便地得到相同结果：

grouped.agg (["min", "max", "count", "mean"]) Out[97]: data 1 data 2 \min max count mean min max count data 1 (- 2.956，- 1.23] - 2.949343 - 1.230179 94 - 1.658818 - 3.399312 1.670835 94 (- 1.23，0.489] - 1.228918 0.488675 598 - 0.329524 - 2.989741 3.260383 598 (0.489，2.208] 0.489965 2.200997 298 1.065727 - 3.745356 2.954439 298 (2.208，3.928] 2.212303 3.927528 10 2.644253 - 1.929776 1.765640 10 mean data 1 (- 2.956，- 1.23] - 0.033333 (- 1.23，0.489] - 0.002622 (0.489，2.208] 0.078249 (2.208，3.928] 0.024750

这些都是长度相等的桶。如果要根据样本分位数得到大小相等的桶，使用 pandas. qcut 即可。我们可以传入 4 作为桶的数量来计算样本分位数，传入 labels=False 以获取分位数的索引（而非区间）：

quartiles_samp = pd.qcut (frame["data 1"], 4, labels=False)

quartiles_samp.head () Out[99]: 0 1

1 3 2 2 3 2 4 3

Name: data 1, dtype: int 64

grouped = frame.groupby (quartiles_samp)

grouped.apply (get_stats) Out[101]:

min max count mean data 1 0 data 1 - 2.949343 - 0.685484 250 - 1.212173 data 2 - 3.399312 2.628441 250 - 0.027045 1 data 1 - 0.683066 - 0.030280 250 - 0.368334 data 2 - 2.630247 3.260383 250 - 0.027845 2 data 1 - 0.027734 0.618965 250 0.295812 data 2 - 3.056990 2.458842 250 0.014450 3 data 1 0.623587 3.927528 250 1.248875 data 2 - 3.745356 2.954439 250 0.115899

# 10.3.3 示例：用指定分组的值填充缺失值

在清洗缺失数据时，有时你会用 dropna 将其删除，而有时则可能想用固定值或由数据本身衍生出的值来填充空（NA）值。这时就得使用 fillna 这个工具了。在下面这个例子中，我用平均值填充空值：

s = pd.Series (np. random. standard_normal (6))

s[::- 2] = np. nan

s Out[104]: 0 NaN 1 0.227290 2 NaN 3 - 2.153545 4 NaN 5 - 0.375842 dtype: float 64

s.fillna (s.mean ()) Out[105]: 0 - 0.767366 1 0.227290 2 - 0.767366 3 - 2.153545 4 - 0.767366 5 - 0.375842 dtype: float 64

假设你需要对不同的分组填充不同的值。一种方法是将数据分组，并使用 apply 和一个能够对各数据块调用 fillna 的函数。下面是一些有关美国若干州的示例数据，将其分为东部地区和西部地区：

states = ['Ohio", "New York", "Vermont", "Florida", ......: "Oregon", "Nevada", "California", "Idaho"] group_key = ["East", "East", "East", "East", ......: "West", "West", "West", "West"] data = pd.Series (np. random. standard_normal (8), index=states) data Out[109]: Ohio 0.329939 New York 0.981994 Vermont 1.105913 Florida - 1.613716 Oregon 1.561587 Nevada 0.406510 California 0.359244 Idaho - 0.614436 dtype: float 64

将其中一些值设置为缺失值：

data["Vermont", "Nevada", "Idaho"]] = np. nan

dataOut[111]: Ohio 0.329939 New York 0.981994 Vermont NANFlorida - 1.613716 Oregon 1.561587 Nevada NANCalifornia 0.359244 Idaho NANdtype: float 64

data.groupby (group_key). size () Out[112]: East 4 West 4 dtype: int 64

data.groupby (group_key). count () Out[113]: East 3 West 2 dtype: int 64

data.groupby (group_key). mean () Out[114]:

East - 0.100594 West 0.960416 dtype: float 64

我们可以用分组平均值来填充 NA 值，如下所示：

def fill_mean (group):...: return group.fillna (group.mean ()) data.groupby (group_key). apply (fill_mean) Out[116]: Ohio 0.329939 New York 0.981994 Vermont - 0.100594 Florida - 1.613716 Oregon 1.561587 Nevada 0.960416 California 0.359244 Idaho 0.960416 dtype: float 64

存在另外一种情况，你可能已经在代码中预设了针对各组的填充值。由于分组都具有内置的 name 属性，我们可以这样使用：

fill_values  $=$  {"East": 0.5,"West":- 1} def fill_func (group): return group.fillna (fill_values[group. name]) data.groupby (group_key). apply (fill_func) Out[119]: Ohio 0.329939 New York 0.981994 Vermont 0.500000 Florida - 1.613716 Oregon 1.561587 Nevada - 1.000000 California 0.359244 Idaho - 1.000000 dtype: float 64

# 10.3.4 示例：随机采样和排列

假设你想要从一个大型数据集中随机抽取（进行替换或不替换）样本，以进行蒙特卡罗模拟（Monte Carlo simulation）或其他工作。“抽取”的方式有很多，这里采用的是 Series 的 sample 方法。

为了进行演示，先来创建一副英氏扑克牌：

suits  $=$  ["H"，"S"，"C"，"D"] #红桃 ，黑桃，梅花，方块 card_val  $=$  （list (range (1，11))  $^+$  [10]\*3)\*4 base_names  $=$  ["A"]  $^+$  list (range (2，11))  $^+$  ["J"，"K"，"Q"] cards  $\equiv$  [] for suit in suits: cards.extend (str (num)  $^+$  suit for num in base_names) deck  $=$  pd. Series（card_val, index  $\equiv$  cards)

现在就有了一个长度等于 52 的 Series，其索引包括牌名和牌值，其中的值则是用于 21 点或其他游戏中用于计分的点数（为了简单起见，令"A"的点数为 1）：

In[121]: deck. head（13） Out[121]: AH 1 2 H 2 3 H 3 4 H 4 5 H 5 6 H 6 7 H 7 8 H 8 9 H 9 10 H 10 JH 10 KH 10 QH 10 dtype: int 64

现在，根据我上面所讲的，从整副牌中随机抽出 5 张，代码如下：

In[122]: def draw（deck，  $n = 5$  ）： return deck. sample（n) In[123]:draw（deck) Out[123]: 4 D 4 QH 10 8 S 8 7 D 7 9 C 9 dtype: int 64

假设你想从每种花色中随机抽取两张牌。由于花色是牌名的最后一个字符，因此我们可以使用 apply 据此进行分组：

In[124]: def get_suit（card): #最后一个字母是花色 return card[- 1] In[125]: deck. groupby（get_suit). apply（draw，  $n = 2$  一 Out[125]: C 6 C 6 KC 10

D 7 D 7 3 D 3 H 7 H 7 9 H 9 S 2 S 2 QS 10 dtype: int 64

我们也可以传入 group_keys=False，丢弃外层的花色索引，只保留选取的牌：

deck.groupby (get_suit, group_keys=False). apply (draw,  $\scriptstyle {\bar{\Pi}} = 2$  Out[126]: AC 1 3 C 3 5 D 5 4 D 4 10 H 10 7 H 7 QS 10 7 S 7 dtype: int 64

# 10.3.5 示例：分组加权平均和相关系数

根据 groupby 的“拆分- 应用- 联合"范式，可以进行 DataFrame 的列与列之间或两个 Series 之间的运算，比如分组加权平均。以下面这个数据集为例，它含有分组键、值以及一些权重值：

df  $=$  pd.DataFrame ({"category"：["a"，"a"，"a"， "b"，"b"，"b"，"b"]， "data": np. random. standard_normal (8), "weights": np.random.uniform (size=8)））

df Out[128]: category data weights 0 a - 1.691656 0.955905 1 a 0.511622 0.012745 2 a - 0.401675 0.137009 3 a 0.968578 0.763037 4 b - 1.818215 0.492472 5 b 0.279963 0.832908 6 b - 0.200819 0.658331 7 b - 0.217221 0.612009

然后可以利用 category 计算分组加权平均值：

grouped = df.groupby ("category") def get_wavg (group):    ...: return np.average (group["data"], weights=group["weights"])

grouped.apply (get_wavg) Out[131]: categorya - 0.495807 b - 0.357273 dtype: float 64

另一个例子，考虑一个来自 Yahoo！Finance 的数据集，其中含有几只股票和标准普尔 500 指数（符号 SPX）的收盘价：

close_px = pd. read_csv ("examples/stock_px. csv", parse_dates=True, index_col=0)

close_px.info () <class 'pandas.core.frame.DataFrame'> DatetimeIndex: 2214 entries, 2003- 01- 02 to 2011- 10- 14 Data columns (total 4 columns): # Column Non- Null Count dtype 0 AAPL 2214 non- null float 64 1 MSFT 2214 non- null float 64 2 XOM 2214 non- null float 64 3 SPX 2214 non- null float 64

dtypes: float 64 (4) memory usage: 86.5 KB

close_px.tall (4) Out[134]:

AAPL MSFT XOM SPX 2011- 10- 11 400.29 27.00 76.27 1195.54 2011- 10- 12 402.19 26.96 77.16 1207.25 2011- 10- 13 408.43 27.18 76.37 1203.66 2011- 10- 14 422.00 27.27 78.11 1224.58

这里用到的是 DataFrame 的 info () 方法，可以便捷地获取 DataFrame 内容的全貌信息。

计算由日收益率（通过百分数变化计算）与 SPX 之间的年化相关系数组成的 DataFrame，也许是一个令人感兴趣的任务。下面是一个实现办法，我们先创建一个函数，用它计算每列和"SPX"列对应的相关系数：

def spx_corr (group):    ...: return group.corrwith (group["SPX"])

接下来，我们使用 pct_change 计算 close_px 的百分比变化：

rets = close_px. pct_change (). dropna ()

最后，我们按照年将百分比变化进行分组，可以用一个单行函数返回每个 datetime 标签的 year 属性，这样就从每行标签提取出年份了：

def get_year (x): return x.year by_year  $=$  rets.groupby (get_year) by_year.apply (spx_corr) Out[139]: AAPL MSFT XOM SPX 2003 0.541124 0.745174 0.661265 1.0 2004 0.374283 0.588531 0.557742 1.0 2005 0.467540 0.562374 0.631010 1.0 2006 0.428267 0.406126 0.518514 1.0 2007 0.508118 0.658770 0.786264 1.0 2008 0.681434 0.804626 0.828303 1.0 2009 0.707103 0.654902 0.797921 1.0 2010 0.710105 0.730118 0.839057 1.0 2011 0.691931 0.800996 0.859975 1.0

当然，你还可以计算列与列之间的相关系数。这里，我们计算 Apple 和 Microsoft 的年度相关系数：

def corrAAPLmsft (group): return group["AAPL"]. corr (group["MSFT"]) by_year.apply (corrAAPLmsft) Out[141]: 2003 0.480868 2004 0.259024 2005 0.300093 2006 0.161735 2007 0.417738 2008 0.611901 2009 0.432738 2010 0.571946 2011 0.581987 dtype: float 64

# 10.3.6 示例：分组线性回归

延续上一个例子的主题，只要函数返回的是 pandas 对象或标量值，就可以用 groupby 执行更复杂的分组统计分析。例如，我可以定义如下的 regress 函数（利用 statsmodels 计量经济学库）对各数据块执行普通最小二乘（Ordinary Least Squares，OLS）回归：

import statsmodels. apL as sm def regress (data, yvar  $\equiv$  None, xvars  $\equiv$  None):  $\texttt{Y} =$  data[yvar]  $\times =$  data[xvars]  $\times ["$  intercept"]  $=$  1. result  $=$  sm.OLS (Y, X). fit () return result. params

如果没有安装 statsmodels，则可以使用 conda 进行安装：

conda install statsmodels

现在，为了计算 AAPL 对 SPX 收益率的年化线性回归，执行如下代码：

by_year.apply (regress, yvar="AAPL", xvars=["SPX"]) Out[143]:

SPX intercept 2003 1.195406 0.000710 2004 1.363463 0.004201 2005 1.766415 0.003246 2006 1.645496 0.000980 2007 1.198761 0.003438 2008 0.968016 - 0.001110 2009 0.879103 0.002954 2010 1.052608 0.001261 2011 0.806605 0.001514

# 10.4 分组转换和“展开式”GroupBy 运算

在 10.3 节中，我们学习了用于分组运算和转换的 apply 方法。还有另一个内置的 transform 方法，它类似于 apply，但对能使用的函数有更多限制：

·生成标量值，能传播到分组形状。·生成与输入分组具有相同形状的对象。·无法修改输入。

来看一个简单的示例：

df = pd.DataFrame ({'key': ['a', 'b', 'c'] * 4, 'value': np.arange (12. })

dfOut[145]:

key value 0 a 0.0 1 b 1.0 2 C 2.0 3 a 3.0 4 b 4.0 5 C 5.0 6 a 6.0 7 b 7.0 8 C 8.0 9 a 9.0 10 b 10.0 11 C 11.0

按照键计算分组平均值：

g = df.groupby ('key')['value']g.mean () Out[147]: keya 4.5 b 5.5 c 6.5 Name: value, dtype: float 64

假设我们想创建一个 Series，它的形状与 df['value']相同，但值替换为按照'key'的分组平均值。我们可以向 transform 传入一个计算单个分组平均值的函数：

def get_mean (group): return group.mean ()

g.transform (get_mean)

Out[149]:

0 4.5 1 5.5 2 6.5 3 4.5 4 5.5 5 6.5 6 4.5 7 5.5 8 6.5 9 4.5 10 5.5 11 6.5 Name: value, dtype: float 64

对于内置的聚合函数，我们可以像 GroupBy 的 agg 方法那样，传入函数的字符串别名：

g.transform ('mean') Out[150]: 0 4.5 1 5.5 2 6.5 3 4.5 4 5.5 5 6.5 6 4.5 7 5.5 8 6.5 9 4.5 10 5.5 11 6.5 Name: value, dtype: float 64

类似于 apply，transform 可以使用能返回 Series 的函数，但结果必须与输入具有相同的大小。例如，我们可以使用一个辅助函数将各分组乘以 2：

def times_two (group):    ...: return group * 2

g.transform (times_two)

Out[152]:

0 0.0 1 2.0 2 4.0 3 6.0 4 8.0 5 10.0 6 12.0 7 14.0 8 16.0 9 18.0 10 20.0 11 22.0 Name: value, dtype: float 64

再复杂一点，我们可以计算各个分组的降序排名：

def get_ranks (group):    ...: return group.rank (ascending=False)

g.transform (get_ranks)

Out[154]:

0 4.0 1 4.0 2 4.0 3 3.0 4 3.0 5 3.0 6 2.0 7 2.0 8 2.0 9 1.0 10 1.0 11 1.0

Name: value, dtype: float 64

再考虑一个由简单聚合方法构成的分组转换函数：

def normalize (x):    ...: return (x - x.mean ()) / x.std ()

对于这种情况，使用 transform 或 apply 都可以获得等价的结果：

g.transform (normalize) Out[156]: 0 - 1.161895 1 - 1.161895

2 - 1.161895  3 - 0.387298  4 - 0.387298  5 - 0.387298  6 0.387298  7 0.387298  8 0.387298  9 1.161895  10 1.161895  11 1.161895  Name: value, dtype: float 64

g.apply (normalize)  Out[157]:  0 - 1.161895  1 - 1.161895  2 - 1.161895  3 - 0.387298  4 - 0.387298  5 - 0.387298  6 0.387298  7 0.387298  8 0.387298  9 1.161895 10 1.161895 11 1.161895  Name: value, dtype: float 64

'mean'或'sum'等内置聚合函数通常比一般的 apply 函数快得多。这些函数在与 transform 配合使用时，也存在“高速运行路径”。我们借此可以实现展开式分组运算：

g.transform ('mean')

Out[158]:

0 4.5 1 5.5 2 6.5 3 4.5 4 5.5 5 6.5 6 4.5 7 5.5 8 6.5 9 4.5 10 5.5 11 6.5

Name: value, dtype: float 64

normalized  $=$  （df['value'] - g.transform ('mean'))/g.transform ('std')

normalized Out[160]: 0 - 1.161895 1 - 1.161895

2 - 1.161895 3 - 0.387298 4 - 0.387298 5 - 0.387298 6 0.387298 7 0.387298 8 0.387298 9 1.161895 10 1.161895 11 1.161895 Name: value, dtype: float 64

这里，我们直接在多个 Groupby 运算输出的结果之间进行算数运算，而非编写一个函数，再将其传给 groupby（..）. apply。这就是“展开式"要表达的含义。

展开式分组运算中可能包含多个分组聚合运算，但向量化运算的整体收益往往更高。

# 10.5 透视表和交叉表

10.5 透视表和交叉表透视表是各种电子表格程序和其他数据分析软件中常见的数据汇总工具。它根据一个或多个键对数据进行聚合，并根据行和列上的分组键将数据分配到矩形区域中。在 Python 和 pandas 中，可以通过本章所介绍的 groupby 功能，结合使用层次化索引的重塑操作来制作透视表。DataFrame 有一个 pivot_table 方法，此外还有一个顶级的 pandas. pivot_table 函数。除了为 groupby 提供便利的接口，pivot_table 还可以添加分项汇总，也称作差额。

回到小费数据集，假设你想计算分组平均数（pivot_table 的默认聚合类型），并在行方向上根据 day 和 smoker 排列：

<table><tr><td colspan="7">tips.head ()</td></tr><tr><td colspan="7">Out[161]:</td></tr><tr><td>total_bill</td><td>tip smoker</td><td>day</td><td>time</td><td>size</td><td>tip_pct</td><td></td></tr><tr><td>0</td><td>16.99</td><td>1.01</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr><tr><td>1</td><td>10.34</td><td>1.66</td><td>No</td><td>Sun</td><td>Dinner</td><td>3</td></tr><tr><td>2</td><td>21.01</td><td>3.50</td><td>No</td><td>Sun</td><td>Dinner</td><td>3</td></tr><tr><td>3</td><td>23.68</td><td>3.31</td><td>No</td><td>Sun</td><td>Dinner</td><td>2</td></tr><tr><td>4</td><td>24.59</td><td>3.61</td><td>No</td><td>Sun</td><td>Dinner</td><td>4</td></tr><tr><td colspan="7">tips. pivot_table (index=[&quot; day&quot;, &quot; smoker&quot;])</td></tr><tr><td colspan="7">Out[162]:</td></tr><tr><td colspan="7">day smoker</td></tr><tr><td rowspan="2">Fri</td><td>No</td><td>2.250000</td><td>2.812500</td><td>0.151650</td><td>18.420000</td><td></td></tr><tr><td>Yes</td><td>2.066667</td><td>2.714000</td><td>0.174783</td><td>16.813333</td><td></td></tr><tr><td rowspan="2">Sat</td><td>No</td><td>2.555556</td><td>3.102889</td><td>0.158048</td><td>19.661778</td><td></td></tr><tr><td>Yes</td><td>2.476190</td><td>2.875476</td><td>0.147906</td><td>21.276667</td><td></td></tr><tr><td>Sun</td><td>No</td><td>2.929825</td><td>3.167895</td><td>0.160113</td><td>20.506667</td><td></td></tr><tr><td colspan="7">Yes</td></tr><tr><td rowspan="2">Thur</td><td>No</td><td>2.488889</td><td>2.673778</td><td>0.160298</td><td>17.113111</td><td></td></tr><tr><td>Yes</td><td>2.352941</td><td>3.030000</td><td>0.163863</td><td>19.190588</td><td></td></tr></table>

可以通过 groupby 直接实现，即 tips. groupby（["day"，"smoker"]）. mean ()。现在，假设我们只想对 tip_pct 和 size 求平均值，并根据 time 进行分组。我把 smoker 放到列上，把 time 和 day 放到行上：

<table><tr><td colspan="6">tips. pivot_table (index=[&quot; time&quot;, &quot; day&quot;], columns=&quot; smoker&quot;, values= [[tip_pct&quot;, &quot;size&quot;\|tip_pct&quot;, &quot;size&quot;]])</td></tr><tr><td colspan="6">Out[163]:</td></tr><tr><td></td><td colspan="2">size</td><td colspan="3">tip_pct</td></tr><tr><td>smoker</td><td>No</td><td>Yes</td><td>No</td><td>Yes</td><td></td></tr><tr><td>time day</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Dinner Fri</td><td>2.000000</td><td>2.222222</td><td>0.139622</td><td>0.165347</td><td></td></tr><tr><td>Sat</td><td>2.555556</td><td>2.476190</td><td>0.158048</td><td>0.147906</td><td></td></tr><tr><td>Sun</td><td>2.929825</td><td>2.578947</td><td>0.160113</td><td>0.187250</td><td></td></tr><tr><td>Thur</td><td>2.000000</td><td>NaN</td><td>0.159744</td><td>NaN</td><td></td></tr><tr><td>Lunch Fri</td><td>3.000000</td><td>1.833333</td><td>0.187735</td><td>0.188937</td><td></td></tr><tr><td>Thur</td><td>2.500000</td><td>2.352941</td><td>0.160311</td><td>0.163863</td><td></td></tr></table>

通过传入 margins=True 添加分项汇总，我们还可以对这个表进行扩充。这会添加标签为 All 的行和列，对应的值为单行或单列中所有数据的分组统计值：

tips. pivot_table (index  $\equiv$  ["time"，"day"], columns  $\equiv$  "smoker"， values  $\equiv$  ["tip_pct"，"size"], margins  $\equiv$  True) Out[164]: size tip_pct smoker No Yes All No Yes All time day Dinner Fri 2.000000 2.222222 2.166667 0.139622 0.165347 0.158916 Sat 2.555556 2.476190 2.517241 0.158048 0.147906 0.153152 Sun 2.929825 2.578947 2.842105 0.160113 0.187250 0.166897 Thur 2.000000 NaN 2.000000 0.159744 NaN 0.159744 Lunch Fri 3.000000 1.833333 2.000000 0.187735 0.188937 0.188765 Thur 2.500000 2.352941 2.459016 0.160311 0.163863 0.161301 All 2.668874 2.408602 2.569672 0.159328 0.163196 0.160803

这里，All 的值为平均值，没有考虑吸烟者与非吸烟者（All 列），也没有考虑行上（All 行）的任意两个分组层级。

要使用 mean 以外的其他聚合函数，可以将其传给 aggfunc 关键字参数。例如，使用"count"或 len ("count"会排除分组数据计数中的空值，而 len 不会）可以得到有关分组大小的交叉表（计数或频率）：

tips. pivot_table (index  $\equiv$  ["time"，"smoker"], columns  $\equiv$  "day" values  $\equiv$  "tip_pct"，aggfunc  $\equiv$  len，margins  $\equiv$  True) Out[165]: day Fri Sat Sun Thur All time smoker Dinner No 3.0 45.0 57.0 1.0 106 Yes 9.0 42.0 19.0 NaN 70 Lunch No 1.0 NaN NaN 44.0 45 Yes 6.0 NaN NaN 17.0 23 All 19.0 87.0 76.0 62.0 244

如果某些组合为空（也就是 NA），可以传入 fill_value 以填充值：

<table><tr><td colspan="7">tips. pivot_table (index=[&quot; time&quot;,&quot; size&quot;,&quot; smoker&quot;], columns=&quot; day&quot;, values=&quot; tip_pct&quot;, fill_value=0)</td></tr><tr><td colspan="7">Out[166]:</td></tr><tr><td>day</td><td>Fri</td><td>Sat</td><td>Sun</td><td>Thur</td><td></td><td></td></tr><tr><td>time</td><td>size smoker</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="5">Dinner</td><td rowspan="2">1</td><td>No</td><td>0.000000</td><td>0.137931</td><td>0.000000</td><td>0.000000</td></tr><tr><td>Yes</td><td>0.000000</td><td>0.325733</td><td>0.000000</td><td>0.000000</td></tr><tr><td rowspan="2">2</td><td>No</td><td>0.139622</td><td>0.162705</td><td>0.168859</td><td>0.159744</td></tr><tr><td>Yes</td><td>0.171297</td><td>0.148668</td><td>0.207893</td><td>0.000000</td></tr><tr><td>3</td><td>No</td><td>0.000000</td><td>0.154661</td><td>0.152663</td><td>0.000000</td></tr><tr><td>...</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="5">Lunch</td><td>3</td><td>Yes</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.204952</td></tr><tr><td rowspan="2">4</td><td>No</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.138919</td></tr><tr><td>Yes</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.155410</td></tr><tr><td>5</td><td>No</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.121389</td></tr><tr><td>6</td><td>No</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.173706</td></tr><tr><td colspan="7">[21 rows x 4 columns]</td></tr></table>

# 表 10-2 总结了 pivot_table 选项。

表 10-2：pivot_table 选项  

<table><tr><td>参数</td><td>说明</td></tr><tr><td>values</td><td>待聚合的列的名称。默认聚合所有数值列</td></tr><tr><td>index</td><td>在结果透视表的行上进行分组的列名或其他分组键</td></tr><tr><td>columns</td><td>在结果透视表的列上进行分组的列名或其他分组键</td></tr><tr><td>aggfunc</td><td>聚合函数或函数列表（默认为 &quot; mean&quot;）；可以是 groupby 上下文中任意有效函数</td></tr><tr><td>fill_value</td><td>用于替换结果表中的缺失值</td></tr><tr><td>dropna</td><td>如果为 True，则不添加条目都为 NA 的列</td></tr><tr><td>margins</td><td>添加行 / 列小计和总计（默认为 False）</td></tr><tr><td>margins_name</td><td>传入 margins=True 时，差额行 / 列使用的名称（默认为 &quot; All&quot;）</td></tr><tr><td>observed</td><td>对于 Categorical 分组键，如果为 True，则只展示分组键中观测到的分类值，而非全部分类</td></tr></table>

# 交叉表：crosstab

交叉表（cross- tabulation，简称 crosstab）是一种用于计算分组频次的特殊透视表。看下面的示例：

from io import StringIO

data = """Sample Nationality Handedness... 1 USA Right- handed... 2 Japan Left- handed... 3 USA Right- handed... 4 Japan Right- handed... 5 Japan Left- handed... 6 Japan Right- handed... 7 USA Right- handed... 8 USA Left- handed... 9 Japan Right- handed... 10 USA Right- handed"""...

data = pd. read_table (StringIO (data), sep="\\s+")

data Out[170]:

Sample Nationality Handedness 0 1 USA Right- handed 1 2 Japan Left- handed 2 3 USA Right- handed 3 4 Japan Right- handed 4 5 Japan Left- handed 5 6 Japan Right- handed 6 7 USA Right- handed 7 8 USA Left- handed 8 9 Japan Right- handed 9 10 USA Right- handed

作为调查分析的一部分，我们可能想根据国籍和用手习惯对这段数据进行统计汇总。虽然可以用 pivot_table 实现该功能，但是 pandas. crosstab 函数会更方便：

pd.crosstab (data["Nationality"], data["Handedness"], margins=True) Out[171]: Handedness Left- handed Right- handed All Nationality Japan 2 3 5 USA 1 4 5 All 3 7 10

crosstab 的前两个参数可以是数组、Series 或者数组列表。对于小费数据，可以如下操作：

pd.crosstab ([tips["time"], tips["day"]], tips["smoker"], margins=True) Out[172]: smoker No Yes All time day Dinner Fri 3 9 12

<table><tr><td rowspan="3"></td><td>Sat</td><td>45</td><td>42</td><td>87</td></tr><tr><td>Sun</td><td>57</td><td>19</td><td>76</td></tr><tr><td>Thur</td><td>1</td><td>0</td><td>1</td></tr><tr><td rowspan="2">Lunch</td><td>Fri</td><td>1</td><td>6</td><td>7</td></tr><tr><td>Thur</td><td>44</td><td>17</td><td>61</td></tr><tr><td>All</td><td></td><td>151</td><td>93</td><td>244</td></tr></table>

# 10.6 总结

掌握 pandas 数据分组工具既有助于数据清洗，也有助于建模或统计分析工作。在第 13 章中，我们会看到在真实数据上使用 groupby 的更多示例。

下一章，我们将关注时间序列数据。